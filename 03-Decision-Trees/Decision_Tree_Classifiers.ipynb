{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#SKLEARN"
      ],
      "metadata": {
        "id": "9pfTDEShVM8c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77ZTxt2YKNOj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "iris = load_iris()\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    iris.data, \n",
        "    iris.target, \n",
        "    test_size=0.2, \n",
        "    random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "clf = DecisionTreeClassifier(max_depth=2, random_state=42)\n",
        "clf.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YF8furBoOVHQ",
        "outputId": "d3256e4c-380b-402d-b132-5eb9bbb55320"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeClassifier(max_depth=2, random_state=42)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import export_graphviz\n",
        "import graphviz\n",
        "\n",
        "dot_data = export_graphviz(clf, \n",
        "    out_file=None, \n",
        "    feature_names=iris.feature_names, \n",
        "    class_names=iris.target_names, \n",
        "    filled=True, rounded=True, \n",
        "    special_characters=True)\n",
        "    \n",
        "graph = graphviz.Source(dot_data)\n",
        "graph.format = \"png\"\n",
        "graph.render(\"decision_tree.png\")\n",
        "graph"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 440
        },
        "id": "_fqFrgSGOYbP",
        "outputId": "cc5cf0a3-5d55-4508-ec2d-32bdb160f1cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<graphviz.files.Source at 0x7f6018171940>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.43.0 (0)\n -->\n<!-- Title: Tree Pages: 1 -->\n<svg width=\"349pt\" height=\"314pt\"\n viewBox=\"0.00 0.00 349.00 314.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 310)\">\n<title>Tree</title>\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-310 345,-310 345,4 -4,4\"/>\n<!-- 0 -->\n<g id=\"node1\" class=\"node\">\n<title>0</title>\n<path fill=\"#fdfffd\" stroke=\"black\" d=\"M204,-306C204,-306 69,-306 69,-306 63,-306 57,-300 57,-294 57,-294 57,-235 57,-235 57,-229 63,-223 69,-223 69,-223 204,-223 204,-223 210,-223 216,-229 216,-235 216,-235 216,-294 216,-294 216,-300 210,-306 204,-306\"/>\n<text text-anchor=\"start\" x=\"65\" y=\"-290.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal length (cm) ≤ 2.45</text>\n<text text-anchor=\"start\" x=\"101\" y=\"-275.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.667</text>\n<text text-anchor=\"start\" x=\"91.5\" y=\"-260.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 120</text>\n<text text-anchor=\"start\" x=\"78.5\" y=\"-245.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [40, 41, 39]</text>\n<text text-anchor=\"start\" x=\"84\" y=\"-230.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 1 -->\n<g id=\"node2\" class=\"node\">\n<title>1</title>\n<path fill=\"#e58139\" stroke=\"black\" d=\"M105,-179.5C105,-179.5 12,-179.5 12,-179.5 6,-179.5 0,-173.5 0,-167.5 0,-167.5 0,-123.5 0,-123.5 0,-117.5 6,-111.5 12,-111.5 12,-111.5 105,-111.5 105,-111.5 111,-111.5 117,-117.5 117,-123.5 117,-123.5 117,-167.5 117,-167.5 117,-173.5 111,-179.5 105,-179.5\"/>\n<text text-anchor=\"start\" x=\"30.5\" y=\"-164.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"start\" x=\"17.5\" y=\"-149.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 40</text>\n<text text-anchor=\"start\" x=\"8\" y=\"-134.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [40, 0, 0]</text>\n<text text-anchor=\"start\" x=\"15\" y=\"-119.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = setosa</text>\n</g>\n<!-- 0&#45;&gt;1 -->\n<g id=\"edge1\" class=\"edge\">\n<title>0&#45;&gt;1</title>\n<path fill=\"none\" stroke=\"black\" d=\"M109.44,-222.91C101.93,-211.65 93.78,-199.42 86.24,-188.11\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"89.07,-186.05 80.61,-179.67 83.25,-189.93 89.07,-186.05\"/>\n<text text-anchor=\"middle\" x=\"75.71\" y=\"-200.48\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">True</text>\n</g>\n<!-- 2 -->\n<g id=\"node3\" class=\"node\">\n<title>2</title>\n<path fill=\"#f5fef9\" stroke=\"black\" d=\"M282,-187C282,-187 147,-187 147,-187 141,-187 135,-181 135,-175 135,-175 135,-116 135,-116 135,-110 141,-104 147,-104 147,-104 282,-104 282,-104 288,-104 294,-110 294,-116 294,-116 294,-175 294,-175 294,-181 288,-187 282,-187\"/>\n<text text-anchor=\"start\" x=\"143\" y=\"-171.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal length (cm) ≤ 4.75</text>\n<text text-anchor=\"start\" x=\"186.5\" y=\"-156.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.5</text>\n<text text-anchor=\"start\" x=\"173.5\" y=\"-141.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 80</text>\n<text text-anchor=\"start\" x=\"160\" y=\"-126.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 41, 39]</text>\n<text text-anchor=\"start\" x=\"162\" y=\"-111.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 0&#45;&gt;2 -->\n<g id=\"edge2\" class=\"edge\">\n<title>0&#45;&gt;2</title>\n<path fill=\"none\" stroke=\"black\" d=\"M163.56,-222.91C169.43,-214.1 175.7,-204.7 181.76,-195.61\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"184.85,-197.28 187.49,-187.02 179.03,-193.4 184.85,-197.28\"/>\n<text text-anchor=\"middle\" x=\"192.39\" y=\"-207.84\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">False</text>\n</g>\n<!-- 3 -->\n<g id=\"node4\" class=\"node\">\n<title>3</title>\n<path fill=\"#3ee684\" stroke=\"black\" d=\"M194,-68C194,-68 97,-68 97,-68 91,-68 85,-62 85,-56 85,-56 85,-12 85,-12 85,-6 91,0 97,0 97,0 194,0 194,0 200,0 206,-6 206,-12 206,-12 206,-56 206,-56 206,-62 200,-68 194,-68\"/>\n<text text-anchor=\"start\" x=\"110\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.053</text>\n<text text-anchor=\"start\" x=\"104.5\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 37</text>\n<text text-anchor=\"start\" x=\"95\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 36, 1]</text>\n<text text-anchor=\"start\" x=\"93\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 2&#45;&gt;3 -->\n<g id=\"edge3\" class=\"edge\">\n<title>2&#45;&gt;3</title>\n<path fill=\"none\" stroke=\"black\" d=\"M188.81,-103.73C183.29,-94.97 177.45,-85.7 171.91,-76.91\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"174.78,-74.89 166.48,-68.3 168.85,-78.63 174.78,-74.89\"/>\n</g>\n<!-- 4 -->\n<g id=\"node5\" class=\"node\">\n<title>4</title>\n<path fill=\"#9253e8\" stroke=\"black\" d=\"M329,-68C329,-68 236,-68 236,-68 230,-68 224,-62 224,-56 224,-56 224,-12 224,-12 224,-6 230,0 236,0 236,0 329,0 329,0 335,0 341,-6 341,-12 341,-12 341,-56 341,-56 341,-62 335,-68 329,-68\"/>\n<text text-anchor=\"start\" x=\"247\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.206</text>\n<text text-anchor=\"start\" x=\"241.5\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 43</text>\n<text text-anchor=\"start\" x=\"232\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 5, 38]</text>\n<text text-anchor=\"start\" x=\"234\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 2&#45;&gt;4 -->\n<g id=\"edge4\" class=\"edge\">\n<title>2&#45;&gt;4</title>\n<path fill=\"none\" stroke=\"black\" d=\"M239.82,-103.73C245.26,-94.97 251.01,-85.7 256.48,-76.91\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"259.52,-78.64 261.82,-68.3 253.57,-74.95 259.52,-78.64\"/>\n</g>\n</g>\n</svg>\n"
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1ASP-J0P-Ch",
        "outputId": "6546ac1d-889c-4da0-d36e-a456fa276724"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9666666666666667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Decision Tree Without SKLEARN"
      ],
      "metadata": {
        "id": "HJiE2FD0Spqm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Step 1: Define the dataset\n",
        "# We will create a small dataset with 10 samples and 4 features\n",
        "# The last column contains the class labels, which are either 0 or 1\n",
        "dataset = np.array([[2.0, 1.0, 1.0, 0],\n",
        "                    [2.0, 1.0, 2.0, 0],\n",
        "                    [2.0, 1.0, 2.0, 1],\n",
        "                    [2.0, 1.0, 1.0, 1],\n",
        "                    [2.0, 1.0, 1.0, 0],\n",
        "                    [1.0, 1.0, 1.0, 0],\n",
        "                    [1.0, 1.0, 2.0, 0],\n",
        "                    [1.0, 1.0, 2.0, 1],\n",
        "                    [1.0, 2.0, 2.0, 1],\n",
        "                    [1.0, 2.0, 1.0, 1]])\n",
        "\n",
        "# Step 2: Define the decision tree function\n",
        "def decision_tree(dataset, max_depth):\n",
        "    # Check if all samples in the dataset belong to the same class\n",
        "    if len(set(dataset[:, -1])) == 1:\n",
        "        return dataset[0][-1]\n",
        "    # Check if the maximum depth of the tree has been reached\n",
        "    if max_depth == 0:\n",
        "        return np.round(np.mean(dataset[:, -1]))\n",
        "    \n",
        "    # Find the best feature to split the dataset\n",
        "    num_features = dataset.shape[1] - 1\n",
        "    best_feature = None\n",
        "    best_gain = 0\n",
        "    for feature in range(num_features):\n",
        "        feature_values = dataset[:, feature]\n",
        "        unique_values = set(feature_values)\n",
        "        gain = 0\n",
        "        for value in unique_values:\n",
        "            # Split the dataset based on the current feature and value\n",
        "            sub_dataset = dataset[feature_values == value]\n",
        "            # Calculate the information gain for the current split\n",
        "            prob = len(sub_dataset) / float(len(dataset))\n",
        "            gain += prob * entropy(sub_dataset)\n",
        "        # Update the best feature and gain if the current gain is higher\n",
        "        if gain > best_gain:\n",
        "            best_feature = feature\n",
        "            best_gain = gain\n",
        "            \n",
        "    # Create a new node for the decision tree with the best feature\n",
        "    decision_tree = {'feature': best_feature}\n",
        "    # Split the dataset based on the best feature and its unique values\n",
        "    feature_values = dataset[:, best_feature]\n",
        "    unique_values = set(feature_values)\n",
        "    for value in unique_values:\n",
        "        sub_dataset = dataset[feature_values == value]\n",
        "        # Recursively build the decision tree for each sub-dataset\n",
        "        decision_tree[value] = decision_tree(sub_dataset, max_depth - 1)\n",
        "        \n",
        "    return decision_tree\n",
        "\n",
        "# Step 3: Define the entropy function\n",
        "def entropy(dataset):\n",
        "    # Count the number of samples in each class\n",
        "    num_samples = len(dataset)\n",
        "    num_class0 = len(dataset[dataset[:, -1] == 0])\n",
        "    num_class1 = len(dataset[dataset[:, -1] == 1])\n",
        "    \n",
        "    # Calculate the entropy of the dataset\n",
        "    if num_class0 == 0 or num_class1 == 0:\n",
        "        entropy = 0\n",
        "    else:\n",
        "        prob_class0 = num_class0 / float(num_samples)\n",
        "        prob_class1 = num_class1 / float(num_samples)\n",
        "\n",
        "        # calculate entropy for the current split\n",
        "        entropy = -prob_class0 * np.log2(prob_class0) - prob_class1 * np.log2(prob_class1)\n",
        "\n",
        "        # calculate information gain for the current split\n",
        "        info_gain = parent_entropy - sum(prob_split) * entropy.sum() / n_samples\n",
        "\n",
        "        # return the feature index and information gain for the best split\n",
        "        return best_feature, info_gain"
      ],
      "metadata": {
        "id": "IJ7aAoZaStOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "Next, we need to implement the actual decision tree model. We will define a Node class that represents a node in the decision tree, with the following attributes:\n",
        "\n",
        "feature: the feature index that this node splits on (if this is a leaf node, feature is None)\n",
        "threshold: the threshold value for the feature split (if this is a leaf node, threshold is None)\n",
        "left_child: the left child node (if this is a leaf node, left_child is None)\n",
        "right_child: the right child node (if this is a leaf node, right_child is None)\n",
        "class_label: the predicted class label for this node (if this is not a leaf node, class_label is None)\n",
        "We will also define a DecisionTreeClassifier class that represents the decision tree model, with the following methods:\n",
        "\n",
        "__init__(): initializes the decision tree with the specified hyperparameters\n",
        "fit(): fits the decision tree to the training data\n",
        "predict(): predicts the class labels for the test data\n",
        "Here's the code for the Node class:\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "C5hUlueJTN7Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Node:\n",
        "    def __init__(self, feature=None, threshold=None, left_child=None, right_child=None, class_label=None):\n",
        "        self.feature = feature\n",
        "        self.threshold = threshold\n",
        "        self.left_child = left_child\n",
        "        self.right_child = right_child\n",
        "        self.class_label = class_label"
      ],
      "metadata": {
        "id": "HbSMolrSTC90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# And here's the code for the DecisionTreeClassifier class:\n",
        "class DecisionTreeClassifier:\n",
        "    def __init__(self, max_depth=None, min_samples_split=2, min_samples_leaf=1):\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.min_samples_leaf = min_samples_leaf\n",
        "        self.tree_ = None\n",
        "    \n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        self.tree_ = self._grow_tree(X, y)\n",
        "        \n",
        "    def predict(self, X):\n",
        "        return np.array([self._predict_tree(x, self.tree_) for x in X])\n",
        "    \n",
        "    def _grow_tree(self, X, y, depth=0):\n",
        "        n_samples, n_features = X.shape\n",
        "        n_class = len(np.unique(y))\n",
        "        \n",
        "        # stop growing the tree if any of the stopping criteria are met\n",
        "        if (self.max_depth is not None and depth >= self.max_depth\n",
        "            or len(X) < self.min_samples_split\n",
        "            or len(np.unique(y)) == 1):\n",
        "            return Node(class_label=np.argmax(np.bincount(y)))\n",
        "        \n",
        "        # find the best feature and threshold for the current split\n",
        "        best_feature, best_threshold = self._best_split(X, y)\n",
        "        \n",
        "        # stop growing the tree if no split improves information gain\n",
        "        if best_feature is None:\n",
        "            return Node(class_label=np.argmax(np.bincount(y)))\n",
        "        \n",
        "        # split the data based on the best feature and threshold\n",
        "        left_idxs = X[:, best_feature] < best_threshold\n",
        "        right_idxs = X[:, best_feature] >= best_threshold\n",
        "        left_tree = self._grow_tree(X[left_idxs], y[left_idxs], depth+1)\n",
        "        right_tree =self._grow_tree(X[right_idxs], y[right_idxs], depth+1)\n",
        "        # If this split is better than the previous one, update the best split\n",
        "        if info_gain > best_info_gain:\n",
        "            best_info_gain = info_gain\n",
        "            best_split_feature = feature_idx\n",
        "            best_split_value = split_value\n",
        "    \n",
        "        # Create the sub-trees for the best split\n",
        "        left_tree_X = X[X[:, best_split_feature] <= best_split_value]\n",
        "        left_tree_y = y[X[:, best_split_feature] <= best_split_value]\n",
        "        right_tree_X = X[X[:, best_split_feature] > best_split_value]\n",
        "        right_tree_y = y[X[:, best_split_feature] > best_split_value]\n",
        "        \n",
        "        # Create the current node and add it to the tree\n",
        "        node = {'split_feature': best_split_feature, 'split_value': best_split_value, 'left': None, 'right': None}\n",
        "        node['left'] = build_decision_tree(left_tree_X, left_tree_y, depth+1, max_depth, min_samples_split, min_samples_leaf)\n",
        "        node['right'] = build_decision_tree(right_tree_X, right_tree_y, depth+1, max_depth, min_samples_split, min_samples_leaf)\n",
        "        \n",
        "        return node\n",
        "\n"
      ],
      "metadata": {
        "id": "DdKPzv0aTZxq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code creates the sub-trees for the best split and then creates a node for the current split using a dictionary. The dictionary contains the split feature, the split value, and pointers to the left and right sub-trees. The function then recursively calls itself on the left and right sub-trees until a stopping condition is met.\n",
        "\n",
        "That's the basic outline of the code for building a decision tree using numpy. Of course, there are many ways to improve and optimize this code, but this should give you a good starting point for building your own decision tree classifier using numpy."
      ],
      "metadata": {
        "id": "Lg8Z0kB5UTSX"
      }
    }
  ]
}